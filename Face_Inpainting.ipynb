{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "X--eckX-mRVz",
        "outputId": "1df145a5-aa19-4153-8049-b096826df562"
      },
      "outputs": [],
      "source": [
        "!pip -q uninstall -y pillow\n",
        "!pip -q install \"pillow<12\" --upgrade\n",
        "\n",
        "!pip -q install --upgrade diffusers transformers accelerate safetensors opencv-python matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdYgc93imh4f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# TRACK 1: Face Inpainting Benchmark\n",
        "# Models:\n",
        "# 1) SD 1.5 Inpaint\n",
        "# 2) SD 2 Inpaint\n",
        "# 3) SDXL Inpaint\n",
        "# 4) LaMa (GAN baseline)\n",
        "\n",
        "\n",
        "import os, sys, subprocess, importlib\n",
        "import torch, numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "import cv2\n",
        "from diffusers import AutoPipelineForInpainting\n",
        "\n",
        "# ---------- Upload ----------\n",
        "print(\"Upload TWO files: (1) injured image  (2) mask image (white=inpaint)\")\n",
        "up = files.upload()\n",
        "uploaded = list(up.keys())\n",
        "img_path, mask_path = uploaded[0], uploaded[1]\n",
        "\n",
        "# ---------- Settings (T4 SAFE) ----------\n",
        "MAX_SIDE = 512\n",
        "SEED = 42\n",
        "GUIDANCE = 7.0\n",
        "\n",
        "STEPS_SD15 = 28\n",
        "STEPS_SD2  = 28\n",
        "STEPS_SDXL = 15   # keep LOW\n",
        "\n",
        "PROMPT = (\n",
        "    \"a realistic photo of the same person, natural healed skin texture, \"\n",
        "    \"consistent facial features, same identity, same lighting, high detail\"\n",
        ")\n",
        "NEG_PROMPT = (\n",
        "    \"different person, changed identity, distorted face, extra eyes, \"\n",
        "    \"cartoon, blur, artifacts, watermark\"\n",
        ")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype  = torch.float16 if device == \"cuda\" else torch.float32\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def load_rgb(p):\n",
        "    return Image.open(p).convert(\"RGB\")\n",
        "\n",
        "def load_mask(p, size):\n",
        "    m = Image.open(p).convert(\"L\").resize(size, Image.NEAREST)\n",
        "    m = ((np.array(m) > 127).astype(np.uint8) * 255)\n",
        "    return Image.fromarray(m)\n",
        "\n",
        "def resize(img, mask):\n",
        "    w, h = img.size\n",
        "    s = min(MAX_SIDE / max(w, h), 1.0)\n",
        "    nw, nh = int(w*s)//8*8, int(h*s)//8*8\n",
        "    return (\n",
        "        img.resize((nw, nh), Image.LANCZOS),\n",
        "        mask.resize((nw, nh), Image.NEAREST)\n",
        "    )\n",
        "\n",
        "def show(images, titles):\n",
        "    plt.figure(figsize=(20,5))\n",
        "    for i, im in enumerate(images):\n",
        "        plt.subplot(1, len(images), i+1)\n",
        "        plt.imshow(im)\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(titles[i], fontsize=10)\n",
        "    plt.show()\n",
        "\n",
        "def cleanup():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# ---------- Load input ----------\n",
        "image = load_rgb(img_path)\n",
        "mask  = load_mask(mask_path, image.size)\n",
        "image, mask = resize(image, mask)\n",
        "\n",
        "show([image, mask.convert(\"RGB\")], [\"Input\", \"Mask\"])\n",
        "\n",
        "# ---------- Diffusion runner ----------\n",
        "def run_diffusion(model_id, steps):\n",
        "    pipe = AutoPipelineForInpainting.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=dtype,\n",
        "        variant=\"fp16\" if dtype == torch.float16 else None,\n",
        "    )\n",
        "\n",
        "    pipe.enable_attention_slicing()\n",
        "    pipe.enable_model_cpu_offload()\n",
        "\n",
        "    gen = torch.Generator(device).manual_seed(SEED)\n",
        "\n",
        "    out = pipe(\n",
        "        prompt=PROMPT,\n",
        "        negative_prompt=NEG_PROMPT,\n",
        "        image=image,\n",
        "        mask_image=mask,\n",
        "        guidance_scale=GUIDANCE,\n",
        "        num_inference_steps=steps,\n",
        "        generator=gen\n",
        "    ).images[0]\n",
        "\n",
        "    del pipe\n",
        "    cleanup()\n",
        "    return out\n",
        "\n",
        "# ---------- LaMa runner ----------\n",
        "def run_lama(img_pil, mask_pil):\n",
        "    try:\n",
        "        try:\n",
        "            importlib.import_module(\"simple_lama_inpainting\")\n",
        "        except:\n",
        "            subprocess.check_call(\n",
        "                [sys.executable, \"-m\", \"pip\", \"-q\", \"install\", \"simple-lama-inpainting\"]\n",
        "            )\n",
        "\n",
        "        from simple_lama_inpainting import SimpleLama\n",
        "        lama = SimpleLama()\n",
        "\n",
        "        img_np = np.array(img_pil)\n",
        "        mask_np = ((np.array(mask_pil) > 127).astype(np.uint8) * 255)\n",
        "\n",
        "        out = lama(img_np, mask_np)\n",
        "        return Image.fromarray(out)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"LaMa failed → OpenCV fallback:\", str(e)[:120])\n",
        "        img_bgr = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)\n",
        "        mask_np = ((np.array(mask_pil) > 127).astype(np.uint8) * 255)\n",
        "        out = cv2.inpaint(img_bgr, mask_np, 3, cv2.INPAINT_TELEA)\n",
        "        return Image.fromarray(cv2.cvtColor(out, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# ---------- Run models ----------\n",
        "results = {}\n",
        "results[\"Input\"] = image\n",
        "\n",
        "print(\"\\nRunning SD 1.5...\")\n",
        "results[\"SD 1.5 Inpaint\"] = run_diffusion(\n",
        "    \"runwayml/stable-diffusion-inpainting\", STEPS_SD15\n",
        ")\n",
        "\n",
        "print(\"\\nRunning SD 2...\")\n",
        "results[\"SD 2 Inpaint\"] = run_diffusion(\n",
        "    \"sd2-community/stable-diffusion-2-inpainting\", STEPS_SD2\n",
        ")\n",
        "\n",
        "print(\"\\nRunning SDXL...\")\n",
        "results[\"SDXL Inpaint\"] = run_diffusion(\n",
        "    \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\", STEPS_SDXL\n",
        ")\n",
        "\n",
        "print(\"\\nRunning LaMa (GAN baseline)...\")\n",
        "results[\"LaMa (GAN baseline)\"] = run_lama(image, mask)\n",
        "\n",
        "# ---------- Show ----------\n",
        "show(list(results.values()), list(results.keys()))\n",
        "\n",
        "# ---------- Save ----------\n",
        "out_dir = \"/content/track1_outputs\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "for k, v in results.items():\n",
        "    v.save(f\"{out_dir}/{k.lower().replace(' ','_')}.png\")\n",
        "\n",
        "print(\"\\n✅ Saved outputs in:\", out_dir)\n",
        "print(os.listdir(out_dir))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "3qIKf3ZV6Q5E",
        "outputId": "0fdc70bb-8fd5-411d-9f52-7dfb5c037d2b"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_a09a2695-1139-4158-977b-300d01a6080b\", \"track1_results.zip\", 1850937)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "shutil.make_archive(\n",
        "    base_name=\"/content/track1_results\",\n",
        "    format=\"zip\",\n",
        "    root_dir=\"/content/track1_outputs\"\n",
        ")\n",
        "\n",
        "files.download(\"/content/track1_results.zip\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odhZjshW7gNR",
        "outputId": "8aa9546c-4c98-4463-d2d7-5adc4b7c17db"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Install\n",
        "!pip install -q scikit-image pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djoo3TMHAzs3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG0lxnb3A7ai"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_rgb_resized(path, size):\n",
        "    return np.array(\n",
        "        Image.open(path).convert(\"RGB\").resize(size, Image.BILINEAR)\n",
        "    )\n",
        "\n",
        "def load_mask(path):\n",
        "    m = Image.open(path).convert(\"L\")\n",
        "    m = np.array(m)\n",
        "    return (m > 127).astype(np.uint8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTTdVH3HDjGv"
      },
      "outputs": [],
      "source": [
        "mask_img = Image.open(MASK).convert(\"L\")\n",
        "mask = (np.array(mask_img) > 127).astype(np.uint8)\n",
        "H, W = mask.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nySUPC5uDmBm"
      },
      "outputs": [],
      "source": [
        "gt = load_rgb_resized(GT, (W, H))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT_m2v86Do1m"
      },
      "outputs": [],
      "source": [
        "rows = []\n",
        "\n",
        "for name, path in outputs.items():\n",
        "    pred = load_rgb_resized(path, (W, H))\n",
        "\n",
        "    psnr_val = psnr(\n",
        "        gt[mask == 1],\n",
        "        pred[mask == 1],\n",
        "        data_range=255\n",
        "    )\n",
        "\n",
        "    ssim_val = ssim(\n",
        "        gt, pred,\n",
        "        data_range=255,\n",
        "        channel_axis=2,\n",
        "        mask=mask\n",
        "    )\n",
        "\n",
        "    rows.append([name, round(psnr_val,2), round(ssim_val,4)])\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(rows, columns=[\"Model\", \"PSNR ↑\", \"SSIM ↑\"])\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZOspEQSFCW-"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "_df_9['index'].plot(kind='line', figsize=(8, 4), title='index')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ9nxBS9E1dl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7, 2.5))\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "\n",
        "ax.table(\n",
        "    cellText=df.values,\n",
        "    colLabels=df.columns,\n",
        "    loc='center',\n",
        "    cellLoc='center'\n",
        ")\n",
        "\n",
        "plt.savefig(\"/content/metrics_table.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "files.download(\"/content/metrics_table.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7aWszFGE3w3"
      },
      "outputs": [],
      "source": [
        "df.plot(\n",
        "    x=\"Model\",\n",
        "    y=[\"PSNR ↑\", \"SSIM ↑\"],\n",
        "    kind=\"bar\",\n",
        "    figsize=(8, 5)\n",
        ")\n",
        "\n",
        "plt.title(\"Face Inpainting Quantitative Comparison\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.grid(axis=\"y\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"/content/metrics_comparison_graph.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "files.download(\"/content/metrics_comparison_graph.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSerc2g3E6QL"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# TRACK 2: DIFFUSION-BASED FACE RESTORATION (KAGGLE SAFE)\n",
        "# =========================================================\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "\n",
        "# -------- DEVICE --------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "# -------- PATHS --------\n",
        "TRACK1_DIR = \"/kaggle/working/track1_outputs\"\n",
        "TRACK2_DIR = \"/kaggle/working/track2_restored_outputs\"\n",
        "os.makedirs(TRACK2_DIR, exist_ok=True)\n",
        "\n",
        "# -------- LOAD IMAGES --------\n",
        "images = {}\n",
        "for f in os.listdir(TRACK1_DIR):\n",
        "    if f.endswith(\".png\") and f != \"input.png\":\n",
        "        images[f.replace(\".png\",\"\")] = Image.open(\n",
        "            os.path.join(TRACK1_DIR, f)\n",
        "        ).convert(\"RGB\")\n",
        "\n",
        "# -------- LOAD PIPELINE --------\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=dtype\n",
        ").to(device)\n",
        "\n",
        "pipe.enable_attention_slicing()\n",
        "\n",
        "# -------- PROMPTS --------\n",
        "PROMPT = (\n",
        "    \"a realistic photo of the same person, \"\n",
        "    \"natural healed skin, sharp facial features, \"\n",
        "    \"same identity, high detail, realistic lighting\"\n",
        ")\n",
        "\n",
        "NEG_PROMPT = (\n",
        "    \"different person, distorted face, blur, artifacts, \"\n",
        "    \"extra eyes, cartoon, oversharpened\"\n",
        ")\n",
        "\n",
        "# -------- RESTORE FUNCTION --------\n",
        "def restore(img):\n",
        "    return pipe(\n",
        "        prompt=PROMPT,\n",
        "        negative_prompt=NEG_PROMPT,\n",
        "        image=img,\n",
        "        strength=0.25,        # LOW change = restoration\n",
        "        guidance_scale=7.0,\n",
        "        num_inference_steps=20\n",
        "    ).images[0]\n",
        "\n",
        "# -------- RUN RESTORATION --------\n",
        "restored = {}\n",
        "for k, v in images.items():\n",
        "    print(\"Restoring:\", k)\n",
        "    restored[k + \"_restored\"] = restore(v)\n",
        "\n",
        "# -------- SHOW --------\n",
        "plt.figure(figsize=(22,6))\n",
        "i = 1\n",
        "for k in images:\n",
        "    plt.subplot(1, len(images)*2, i)\n",
        "    plt.imshow(images[k]); plt.axis(\"off\"); plt.title(k)\n",
        "    i += 1\n",
        "    plt.subplot(1, len(images)*2, i)\n",
        "    plt.imshow(restored[k+\"_restored\"]); plt.axis(\"off\")\n",
        "    plt.title(k + \" + Restored\")\n",
        "    i += 1\n",
        "plt.show()\n",
        "\n",
        "# -------- SAVE --------\n",
        "for k, v in restored.items():\n",
        "    v.save(os.path.join(TRACK2_DIR, k + \".png\"))\n",
        "\n",
        "print(\"DONE. SAVED IN:\", TRACK2_DIR)\n",
        "print(os.listdir(TRACK2_DIR))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
